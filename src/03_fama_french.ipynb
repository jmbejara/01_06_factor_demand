{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "- This notebook intends to walk through the steps of replicating Table 2.\n",
    "- `df_combo` is the main sample as shown in Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wrds\n",
    "\n",
    "from load_CRSP_fund import load_CRSP_combined_file\n",
    "from load_mflink import load_mflink1\n",
    "\n",
    "import config\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
    "path = Path(OUTPUT_DIR) / \"main_sample.parquet\" \n",
    "df_combo = pd.read_parquet(path)\n",
    "\n",
    "df_crsp = load_CRSP_combined_file()\n",
    "df_mflink1 = load_mflink1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRSP Mutual Fund Data\n",
    "\n",
    "- CRSP data and mflink1 are merged based on `crsp_fundno`to obtain the appropriate `wficn`.\n",
    "- Calculate `mret` and `mtna` for each `wficn`.\n",
    "- The new CRSP data is then merged with `df_combo` by `year` and `wficn` to get the main sample's monthly returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crsp = df_crsp.merge(df_mflink1, how=\"inner\", on=\"crsp_fundno\").reset_index(drop=True)\n",
    "\n",
    "df_crsp.sort_values([\"caldt\", \"wficn\"], inplace=True)\n",
    "df_crsp['mret'] = df_crsp['mret'].fillna(0)\n",
    "df_crsp['lipper_class_name'] = df_crsp['lipper_class_name'].fillna('None')\n",
    "\n",
    "df_crsp = df_crsp[~df_crsp['lipper_class_name'].astype(str).str.contains('International|Fixed Income|Precious Metal', case=False, regex=True)]\n",
    "ret = df_crsp.groupby([\"caldt\", \"wficn\", 'lipper_class_name'])[\"mret\"].mean().reset_index().rename(columns={\"mret\": \"crsp_ret\"})\n",
    "tna = df_crsp.groupby([\"caldt\", \"wficn\", 'lipper_class_name'])[\"mtna\"].sum().reset_index().rename(columns={\"mtna\": \"crsp_tna\"})\n",
    "df_crsp = pd.merge(pd.merge(ret, tna, on=[\"caldt\", \"wficn\", 'lipper_class_name'], how=\"inner\"), \n",
    "                   df_crsp[[\"caldt\", \"wficn\", 'lipper_class_name', 'index_fund_flag']], on=[\"caldt\", \"wficn\", 'lipper_class_name'], how=\"inner\").sort_values([\"caldt\", \"wficn\"])\n",
    "df_crsp = df_crsp.drop_duplicates()\n",
    "df_crsp = df_crsp.rename(columns={\"caldt\": \"date\"})\n",
    "\n",
    "df_crsp['year'] = df_crsp['date'].dt.year.astype('int')\n",
    "df_crsp = pd.merge(df_crsp, df_combo[['year', 'wficn']], on=[\"year\", \"wficn\"], how=\"inner\")\n",
    "\n",
    "df_crsp['date'] = df_crsp['date'].dt.strftime('%Y%m').astype('int')\n",
    "df_crsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "# float_format_func = lambda x: '{:.2f}'.format(x)\n",
    "# df_crsp = df_crsp.rename(columns={'lipper_class_name': 'lipper class','crsp_ret': '$crsp_{ret}$', 'crsp_tna':'$crsp_{TNA}$', 'index_fund_flag':'index fund flag'}) \n",
    "\n",
    "# latexTS_crsp_t2 = df_crsp.tail(10).to_latex(float_format = float_format_func)\n",
    "\n",
    "# path_to_save = f'../output/table_crsp_t2.tex'\n",
    "\n",
    "# with open(path_to_save, 'w') as f: \n",
    "#     f.write(latexTS_crsp_t2)\n",
    "    \n",
    "# df_crsp = df_crsp.rename(columns={'lipper class':'lipper_class_name','$crsp_{ret}$': 'crsp_ret', '$crsp_{TNA}$':'crsp_tna', 'index fund flag':'index_fund_flag'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fama French Factors\n",
    "\n",
    "- Factor returns, `df_ff`, are pulled from Kenneth R. French's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff = pd.read_csv(Path(config.DATA_DIR)/'manual'/'F-F_Research_Data_5_Factors_2x3.csv').drop(['RF'], axis=1)\n",
    "df_mom = pd.read_csv(Path(config.DATA_DIR)/'manual'/'F-F_Momentum_Factor.csv')\n",
    "df_ff = df_ff.merge(df_mom, how='inner', on=['date'])\n",
    "df_ff = df_ff[(df_ff['date'] >= 198001) & (df_ff['date'] <= 201912)]\n",
    "df_ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRSP data and factor returns are merged, and for each fund i in month t, $flow_{i,t}$ is calculated using the formula:\n",
    "\n",
    "$$\n",
    "\\text{flow}_{i,t} = \\frac{\\text{TNA}_{i,t}}{\\text{TNA}_{i,t-1}} \\times (1 + \\text{ret}_{i,t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.merge(df_crsp[df_crsp['date'] <= 201912], df_ff, on=['date'], how=\"outer\").sort_values([\"date\"])\n",
    "flow = df_reg.groupby('wficn').apply(lambda d: d['crsp_tna']/(d['crsp_tna'].shift(1)) - (1+d['crsp_ret'])).reset_index().rename(columns={'level_1': 'index', 0: \"flow\"})\n",
    "flow.set_index('index', inplace=True)\n",
    "df_reg = pd.merge(df_reg, flow[['flow']], left_index=True, right_index=True).sort_values(['wficn', 'date'])\n",
    "df_reg[['crsp_ret', 'flow']] *= 100\n",
    "df_reg.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_reg= df_reg.fillna(0)\n",
    "df_reg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latexTS_df_reg = df_reg.tail(10).to_latex(float_format = float_format_func)\n",
    "\n",
    "# path_to_save = f'../output/table_fama_french_t2.tex'\n",
    "\n",
    "# with open(path_to_save, 'w') as f: \n",
    "#     f.write(latexTS_df_reg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting the mean, std, and percentiles of factor betas across all funds\n",
    "\n",
    "- To replicate Panel A of Table 2, for each fund i in month t, we run the following rolling time-series regression:\n",
    "$$\n",
    "\\text{ret}_{i,t+1-k} = \\alpha_{i,t} + \\beta_{\\text{MKT} i,t} \\times \\text{MKT}_{t+1-k} + \\beta_{\\text{HML} i,t} \\times \\text{HML}_{t+1-k} + \\beta_{\\text{SMB} i,t} \\times \\text{SMB}_{t+1-k} + \\beta_{\\text{MOM} i,t} \\times \\text{MOM}_{t+1-k} + \\beta_{\\text{CMA} i,t} \\times \\text{CMA}_{t+1-k} + \\beta_{\\text{RMW} i,t} \\times \\text{RMW}_{t+1-k} + \\beta_{\\text{flow} i,t} \\times \\text{flow}_{i,t+1-k} + \\epsilon_{i,t,t+1-k}\n",
    "$$\n",
    "where k = 1,2,...,60.\n",
    "\n",
    "- We require a fund should have 60 months of returns data and each rolling window contains 24 monthly observationswe need to run regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def regression(df):\n",
    "    # Pre-allocate list for storing coefficients to minimize memory reallocation\n",
    "    coef_list = []\n",
    "    columns = ['Mkt-RF', 'SMB', 'HML', 'MOM', 'CMA', 'RMW', 'flow']\n",
    "\n",
    "    for fund, data in df.groupby('wficn'):\n",
    "        if len(data) >= 60:\n",
    "            # Pre-select columns outside the loop to reduce computation\n",
    "            X_data = data[columns]\n",
    "            y_data = data['crsp_ret']\n",
    "            \n",
    "            for month in range(len(data) - 59):\n",
    "                for rw in range(len(data) - month - 23):\n",
    "                    # Utilize iloc to efficiently select rolling windows\n",
    "                    X = X_data.iloc[month + rw: month + rw + 24]\n",
    "                    y = y_data.iloc[month + rw: month + rw + 24]\n",
    "                    model = LinearRegression().fit(X, y.values.reshape(-1, 1))\n",
    "                    # Store coefficients directly as a tuple or list in coef_list\n",
    "                    coef_list.append(model.coef_[0])  # [0] unwraps the 2D array into 1D\n",
    "    \n",
    "    # Create DataFrame after collecting all coefficients\n",
    "    beta = pd.DataFrame(coef_list, columns=columns)\n",
    "    return beta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reg = df_reg.sample(frac=0.3, random_state=42)\n",
    "# all_funds = regression(df_reg)\n",
    "# panelA = all_funds.describe().loc[['mean', 'std']].concat(all_funds.quantile(0.05)).concat(all_funds.describe().loc[['25%', '50%', '75%']]).concat(all_funds.quantile(0.95))\n",
    "# panelA = panelA.rename(index={0.05: 'P5', '25\\%': 'P25', '50\\%': 'P50', '75\\%': 'P75', 0.95: 'P95'})\n",
    "# panelA\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_reg is defined and regression is a function that returns a DataFrame\n",
    "df_reg = df_reg.sample(frac=0.3, random_state=42)\n",
    "all_funds = regression(df_reg)\n",
    "\n",
    "# Create a DataFrame with descriptive statistics (mean, std)\n",
    "descriptive_stats = all_funds.describe().loc[['mean', 'std']]\n",
    "\n",
    "# Extract quantiles and transpose them to match the descriptive stats' format\n",
    "quantiles_5 = all_funds.quantile(0.05).to_frame().T  # 5th percentile\n",
    "quantiles_25_50_75 = all_funds.describe().loc[['25%', '50%', '75%']]  # 25th, 50th, 75th percentiles\n",
    "quantiles_95 = all_funds.quantile(0.95).to_frame().T  # 95th percentile\n",
    "\n",
    "# Concatenate all pieces into a single DataFrame\n",
    "panelA = pd.concat([descriptive_stats, quantiles_5, quantiles_25_50_75, quantiles_95])\n",
    "\n",
    "# Rename the index to match desired labels. This is necessary after the concatenation\n",
    "# Because quantiles_5 and quantiles_95 don't automatically get named indices in the concat step\n",
    "panelA.index = ['mean', 'std', 'P5', 'P25', 'P50', 'P75', 'P95']\n",
    "\n",
    "panelA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# panelA = panelA.rename(index={0.05: 'P5', '25%': 'P25', '50%': 'P50', '75%': 'P75', 0.95: 'P95'})\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "float_format_func = lambda x: '{:.2f}'.format(x)\n",
    "\n",
    "latexTS_panelA = panelA.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelA.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting the mean factor betas by Lipper mutual fund classifications\n",
    "\n",
    "- To replicate Panel B of Table 2, we classify funds according to `lipper_class_name`, and then run the regressions again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_growth = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Growth', case=False, regex=True)]\n",
    "df_value = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Value', case=False, regex=True)]\n",
    "df_base = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Base', case=False, regex=True)]\n",
    "df_large_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Large-Cap', case=False, regex=True)]\n",
    "df_mid_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Mid-Cap', case=False, regex=True)]\n",
    "df_small_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Small-Cap', case=False, regex=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth = regression(df_growth)\n",
    "value = regression(df_value)\n",
    "base = regression(df_base)\n",
    "large_cap = regression(df_large_cap)\n",
    "mid_cap = regression(df_mid_cap)\n",
    "small_cap = regression(df_small_cap)\n",
    "panelB = pd.DataFrame({'All': all_funds.mean(), 'Growth': growth.mean(), 'Value': value.mean(), \n",
    "              'Large cap': large_cap.mean(), 'Medium cap': mid_cap.mean(), 'Small cap': small_cap.mean()}).T\n",
    "panelB\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexTS_panelB = panelB.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelB.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting the mean factor betas by index fund status\n",
    "- To replicate Panel C of Table 2, we classify funds according to index fund status.\n",
    "- `index_fund_flag` identifies if a fund is an index fund:\n",
    "- B = index-based fund\n",
    "- D = pure index fund\n",
    "- E = index fund enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('D|B|E', case=False, regex=True)]\n",
    "df_enhanced = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('E', case=False, regex=True)]\n",
    "df_base = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('B', case=False, regex=True)]\n",
    "df_pure = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('D', case=False, regex=True)]\n",
    "df_non_index = df_reg[~df_reg['index_fund_flag'].astype(str).str.contains('D|B|E', case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = regression(df_index)\n",
    "enhanced = regression(df_enhanced)\n",
    "base = regression(df_base)\n",
    "pure = regression(df_pure)\n",
    "non_index = regression(df_non_index)\n",
    "panelC = pd.DataFrame({'All index funds': index.mean(), 'Enhanced': enhanced.mean(), 'Base': base.mean(), \n",
    "              'Pure': pure.mean(), 'All non-index funds': non_index.mean()}).T\n",
    "panelC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexTS_panelC = panelC.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelC.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalculation using data up until the Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff = pd.read_csv(Path(config.DATA_DIR)/'manual'/'F-F_Research_Data_5_Factors_2x3.csv').drop(['RF'], axis=1)\n",
    "df_mom = pd.read_csv(Path(config.DATA_DIR)/'manual'/'F-F_Momentum_Factor.csv')\n",
    "df_ff = df_ff.merge(df_mom, how='inner', on=['date'])\n",
    "df_ff = df_ff[df_ff['date'] >= 202001]\n",
    "\n",
    "df_reg = pd.merge(df_crsp[df_crsp['date'] >= 202001], df_ff, on=['date'], how=\"outer\").sort_values([\"date\"])\n",
    "flow = df_reg.groupby('wficn').apply(lambda d: d['crsp_tna']/(d['crsp_tna'].shift(1)) - (1+d['crsp_ret'])).reset_index().rename(columns={'level_1': 'index', 0: \"flow\"})\n",
    "flow.set_index('index', inplace=True)\n",
    "df_reg = pd.merge(df_reg, flow[['flow']], left_index=True, right_index=True).sort_values(['wficn', 'date'])\n",
    "df_reg[['crsp_ret', 'flow']] *= 100\n",
    "df_reg.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_reg= df_reg.fillna(0)\n",
    "df_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_funds_pre = regression(df_reg)\n",
    "# panelA_pre = all_funds_pre.describe().loc[['mean', 'std']].append(all_funds_pre.quantile(0.05)).append(all_funds_pre.describe().loc[['25%', '50%', '75%']]).append(all_funds_pre.quantile(0.95))\n",
    "# panelA_pre = panelA_pre.rename(index={0.05: 'P5', '25%': 'P25', '50%': 'P50', '75%': 'P75', 0.95: 'P95'})\n",
    "# panelA_pre\n",
    "\n",
    "df_reg_sampled = df_reg.sample(frac=0.3, random_state=42)\n",
    "all_funds_pre = regression(df_reg_sampled)\n",
    "\n",
    "# Create a DataFrame with descriptive statistics (mean, std)\n",
    "descriptive_stats_pre = all_funds_pre.describe().loc[['mean', 'std']]\n",
    "\n",
    "# Extract quantiles and transpose them to match the descriptive stats' format\n",
    "quantiles_5_sampled = all_funds_pre.quantile(0.05).to_frame().T  # 5th percentile\n",
    "quantiles_25_50_75_sampled = all_funds_pre.describe().loc[['25%', '50%', '75%']]  # 25th, 50th, 75th percentiles\n",
    "quantiles_95_sampled = all_funds_pre.quantile(0.95).to_frame().T  # 95th percentile\n",
    "\n",
    "# Concatenate all pieces into a single DataFrame\n",
    "panelA_pre = pd.concat([descriptive_stats_pre, quantiles_5_sampled, quantiles_25_50_75_sampled, quantiles_95_sampled])\n",
    "\n",
    "# Rename the index to match desired labels. This is necessary after the concatenation\n",
    "# Because quantiles_5_sampled and quantiles_95_sampled don't automatically get named indices in the concat step\n",
    "panelA_pre.index = ['mean', 'std', 'P5', 'P25', 'P50', 'P75', 'P95']\n",
    "\n",
    "panelA_pre\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexTS_panelA_pre = panelA_pre.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelA_pre.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelA_pre) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_growth = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Growth', case=False, regex=True)]\n",
    "df_value = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Value', case=False, regex=True)]\n",
    "df_base = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Base', case=False, regex=True)]\n",
    "df_large_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Large-Cap', case=False, regex=True)]\n",
    "df_mid_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Mid-Cap', case=False, regex=True)]\n",
    "df_small_cap = df_reg[df_reg['lipper_class_name'].astype(str).str.contains('Small-Cap', case=False, regex=True)]\n",
    "\n",
    "growth_pre = regression(df_growth)\n",
    "value_pre = regression(df_value)\n",
    "base_pre = regression(df_base)\n",
    "large_cap_pre = regression(df_large_cap)\n",
    "mid_cap_pre = regression(df_mid_cap)\n",
    "small_cap_pre = regression(df_small_cap)\n",
    "panelB_pre = pd.DataFrame({'All': all_funds_pre.mean(), 'Growth': growth_pre.mean(), 'Value': value_pre.mean(), \n",
    "              'Large cap': large_cap_pre.mean(), 'Medium cap': mid_cap_pre.mean(), 'Small cap': small_cap_pre.mean()}).T\n",
    "panelB_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexTS_panelB_pre = panelB_pre.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelB_pre.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelB_pre) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('D|B|E', case=False, regex=True)]\n",
    "df_enhanced = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('E', case=False, regex=True)]\n",
    "df_base = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('B', case=False, regex=True)]\n",
    "df_pure = df_reg[df_reg['index_fund_flag'].astype(str).str.contains('D', case=False, regex=True)]\n",
    "df_non_index = df_reg[~df_reg['index_fund_flag'].astype(str).str.contains('D|B|E', case=False, regex=True)]\n",
    "\n",
    "index_pre = regression(df_index)\n",
    "enhanced_pre = regression(df_enhanced)\n",
    "base_pre = regression(df_base)\n",
    "pure_pre = regression(df_pure)\n",
    "non_index_pre = regression(df_non_index)\n",
    "panelC_pre = pd.DataFrame({'All index funds': index_pre.mean(), 'Enhanced': enhanced_pre.mean(), 'Base': base_pre.mean(), \n",
    "              'Pure': pure_pre.mean(), 'All non-index funds': non_index_pre.mean()}).T\n",
    "panelC_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latexTS_panelC_pre = panelC_pre.to_latex(float_format = float_format_func)\n",
    "\n",
    "path_to_save = f'../output/table_panelC_pre.tex'\n",
    "\n",
    "with open(path_to_save, 'w') as f: \n",
    "    f.write(latexTS_panelC_pre) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
